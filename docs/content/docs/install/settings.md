---
title: Settings
weight: 3
---

## Tweak Your Pipelines-as-Code Setup

Want to customize Pipelines-as-Code?  You can adjust a few things using the `pipelines-as-code` config map, which lives in the `pipelines-as-code` namespace. Let's dive into what you can tweak:

* **`application-name`**:  Give your Pipelines-as-Code instance a friendly name! This name pops up when showing PipelineRun results. If you're using the GitHub App, remember to update the label in your GitHub App settings too.  By default, it's set to `Pipelines-as-Code CI`.

* **`secret-auto-create`**:  Thinking about private repos? This setting is your friend. It automatically creates a secret with a token (generated by the GitHub App) so you can work with those private repositories. Good news: it's turned on by default.

* **`secret-github-app-token-scoped`**:  GitHub App users, listen up! Pipelines-as-Code can create temporary, scoped tokens for each event it handles.  "Scoped" means the token only works for the repository where the event came from, not *all* repos the app has access to.

    Why is this cool? Imagine you've installed the Pipelines-as-Code app in a GitHub organization with both public and private repos. You might not want every user in the org to have blanket access to the private stuff.  By scoping the token, you keep things secure – users can only mess with the repo related to the event, not poke around in private areas they shouldn't.

    However, if you *do* trust everyone in your organization with all repos, or if you're not using the GitHub App organization-wide, you can set this option to `false`.  Basically, it relaxes the security a bit.

* **`secret-github-app-scope-extra-repos`**:  Okay, so maybe you want *some* token scoping, but you also need access to a few extra private repos (where the GitHub App is also installed). This setting lets you list those exceptions.

    Think of it like giving access to a few "bonus" private repos.  This is handy if you need to fetch remote files from other private repos within your organization.

    Important note: This only works if all the repos are under the same GitHub App installation.

    List repos as `owner/repo`, separated by commas, like this:

    ```yaml
    secret-github-app-token-scoped: "owner/private-repo1, org/repo2"
    ```

* **`remote-tasks`**: Want to use tasks defined in external files (referenced in PipelineRun annotations)?  This setting allows Pipelines-as-Code to fetch those remote tasks. It's enabled by default, making it easy to reuse tasks.

* **`bitbucket-cloud-check-source-ip`**:  Bitbucket Cloud is a bit different – it doesn't have secrets in the same way GitHub does. To keep things secure with public Bitbucket repos, Pipelines-as-Code checks if requests are coming from authorized Atlassian IP ranges (you can find them [here](https://ip-ranges.atlassian.com/)).  This only kicks in for public Bitbucket when you *haven't* set a custom provider URL in your Repository spec.

    If you want to turn this IP check *off*, you can, but be warned: it opens up a potential security hole. Someone could try to sneak malicious code into your PipelineRun and trick the system into running it by pretending to be an authorized user.  Just something to keep in mind! This feature is on by default for safety.

* **`bitbucket-cloud-additional-source-ip`**: Need to allow requests from extra IPs for Bitbucket Cloud?  No problem!  You can add specific IPs (like `127.0.0.1`) or whole networks (`127.0.0.0/16`).  Just separate multiple entries with commas.

* **`max-keep-run-upper-limit`**:  Want to set a hard limit on how many PipelineRuns are kept around?  This setting lets you define the *maximum* value for the `max-keep-run` annotation in your PipelineRuns. If someone tries to set `max-keep-run` higher than this limit, Pipelines-as-Code will just use this upper limit instead for cleanup. Think of it as a safety net to prevent runaway run history.

* **`default-max-keep-runs`**:  Want a default number of PipelineRuns to keep for *all* your pipelines, unless specified otherwise?  Set this!  If you define this, any PipelineRun *without* a `max-keep-runs` annotation will automatically use this default value.  Easy peasy for consistent cleanup.

* **`auto-configure-new-github-repo`**:  Got a brand new GitHub repo?  Want Pipelines-as-Code to automatically set things up for it?  Turning this on means that when Pipelines-as-Code sees a new repo URL in a webhook payload, it'll create a namespace and a Repository CR for you.  Less manual setup, more automation!

    This is off by default and only works with GitHub Apps.

{{< hint info >}}
**Important:** If you're using a GitHub App and want this feature, double-check your GitHub App settings to make sure it's subscribed to the `repository` event.
{{< /hint >}}

* **`auto-configure-repo-namespace-template`**:  If you've enabled `auto-configure-new-github-repo`, you can even customize how the namespace for new repos is named! By default, it uses the format `{{repo_name}}-pipelines`.

    Want to get fancy? You can use these variables in your template:

    * `{{repo_owner}}`: The repository owner (like your GitHub username or organization name).
    * `{{repo_name}}`: The name of the repository itself.

    For example, if you set the template to `{{repo_owner}}-{{repo_name}}-ci`, a repo like `https://github.com/owner/repo` would get the namespace `owner-repo-ci`.  Neat, huh?

* **`remember-ok-to-test`**:  Using `ok-to-test` on pull requests?  This setting makes things a bit smoother. If it's `true` and someone uses `ok-to-test` on a PR, then any subsequent pushes to that PR (new commits, amends, etc.) will automatically trigger CI again.

    If you want to require `ok-to-test` on *every* iteration (maybe for stricter security?), you can set this to `false`. Currently, this works with GitHub and Gitea.

### Tekton Hub Goodness

Pipelines-as-Code can grab tasks from remote places, and by default, it's set up to use the [public Tekton Hub](https://hub.tekton.dev/). But hey, maybe you've got your own private hub?  No problem, you can point Pipelines-as-Code to it with these settings:

* **`hub-url`**: This is the base URL for your Tekton Hub API. The default is the public hub: <https://api.hub.tekton.dev/v1>.  If you're using your own, just swap this out.

* **`hub-catalog-name`**:  The name of the catalog in your Tekton Hub.  Defaults to `tekton`.

* **Multiple Hubs? We Got You.** You can even configure *multiple* hubs!  Use this pattern:

    ```yaml
    catalog-1-id: "custom"
    catalog-1-name: "tekton"
    catalog-1-url: "https://api.custom.hub/v1"
    ```

    Users can then fetch tasks from your custom hub by prefixing the task name with `custom://`.  For example, `custom://my-task`.

    You can add as many custom hubs as you need, just keep incrementing the number in `catalog-NUMBER`.

    Important: Pipelines-as-Code won't try to fall back to the default hub or other custom hubs if a task isn't found in the specified catalog.  If it can't find the task, the Pull Request will be marked as failed.

### Error Hunting Made Easier

Pipelines-as-Code is pretty good at spotting when a PipelineRun goes wrong.  It can even show you a snippet of the error from the logs.

If you're using the GitHub App, it goes a step further! It tries to find error messages in the container logs and display them as [GitHub annotations](https://github.blog/2018-12-14-introducing-check-runs-and-annotations/) right on your Pull Request.  Handy for quick debugging!

Here are the settings to tweak this error detection magic:

* **`error-log-snippet`**:  Want to see that little log snippet when a task fails?  This setting turns that feature on or off.  Because of how different Git providers' APIs work, it usually shows the last 3 lines of logs from the first container in the first task that threw an error in the PipelineRun.

    And for extra security, if it finds any secrets in those log lines (secrets you've attached to the PipelineRun), it'll replace them with `******` so you don't accidentally leak sensitive info.

* **`error-detection-from-container-logs`**:  Want Pipelines-as-Code to dig into container logs and find error messages to display as annotations in your Pull Request (GitHub Apps only)?  This setting enables or disables that.

* **`error-detection-max-number-of-lines`**:  When Pipelines-as-Code is looking through container logs for errors (using `error-log-snippet`), this setting controls how many lines it grabs.  Increasing this might use a bit more memory.  Use `-1` for unlimited lines (if you really want to be thorough).

* **`error-detection-simple-regexp`**:  By default, error detection is set up for simple error outputs, like the kind you see from GCC or Make (and many linters and command-line tools).

   For example, it understands errors that look like this:

   ```console
   test.js:100:10: an error occurred
   ```

   Pipelines-as-Code will spot this, and show it as an annotation on your pull request, pointing to `test.js`, line 100.  Pretty slick!

   You can actually customize the regular expression used for error detection. If you do, you'll need to use regexp groups to tell Pipelines-as-Code where to find the filename, line number, and error message.  The groups it looks for are: `<filename>`, `<line>`, and `<error>`.

### Log Reporting - Your Way

Pipelines-as-Code can send task logs to different consoles, giving you flexibility in how you monitor things.  It supports:

* **OpenShift Console**: If you're on OpenShift, Pipelines-as-Code will automatically detect the console and link task logs to its public URL.  Easy peasy!

* **[Tekton Dashboard](https://tekton.dev/docs/dashboard/)**: Using Tekton Dashboard?  Just set the `tekton-dashboard-url` setting to your dashboard's URL, and PipelineRun statuses and task logs will show up there.

* **Your Own Custom Console/Dashboard**:  Got a custom dashboard you prefer?  You can configure Pipelines-as-Code to link to it!  Use these settings:

    * **`custom-console-name`**:  Give your custom console a name, like `MyCorp Console`.

    * **`custom-console-url`**: The root URL of your custom console (e.g., `https://mycorp.com`).

    * **`custom-console-url-namespace`**:  The URL to view namespace details in your console. You can use variables here (like those in [Authoring PipelineRuns](../authoringprs) docs) plus `{{ namespace }}` (the namespace where the PipelineRun runs). Example: `https://mycorp.com/ns/{{ namespace }}`.

    * **`custom-console-url-pr-details`**:  The URL to see PipelineRun details.  This link is shown when a PipelineRun starts, so users can follow along, or later to see results.  Again, you can use standard PipelineRun variables, plus `{{ namespace }}` and `{{ pr }}` (the PipelineRun name). Example: `https://mycorp.com/ns/{{ namespace }}/pipelinerun/{{ pr }}`.

        You can even use [custom parameters](../guide/repositorycrd/#custom-parameter-expansion) from your Repository CR in these URLs!  For instance, if your Repo CR has:

        ```yaml
        [...]
        spec:
         params:
           - name: custom
             value: value
        ```

        And your `custom-console-url-pr-details` is set to:

        `https://mycorp.com/ns/{{ namespace }}/{{ custom }}`

        Then `{{ custom }}` in the URL will be replaced with `value`. This is great for adding specific info like user UUIDs from your Repo CR to the console links.

    * **`custom-console-url-pr-tasklog`**:  The URL to view task logs within a PipelineRun. This link is shown when task results are posted.  You can use Repo CR custom parameters, plus the variables from `custom-console-url-pr-details`, and these extra ones: `{{ task }}` (task name), `{{ pod }}` (pod name), and `{{ firstFailedStep }}` (name of the first failed step). Example: `https://mycorp.com/ns/{{ namespace }}/pipelinerun/{{ pr }}/logs/{{ task }}#{{ pod }}-{{ firstFailedStep }}`.

## Pipelines-as-Code Info - Publicly Available

There's a config map that *anyone* with access to your cluster can read to get info about Pipelines-as-Code.  This config map is automatically created by the [OpenShift Pipelines Operator](https://docs.openshift.com/container-platform/latest/cicd/pipelines/understanding-openshift-pipelines.html) or when you use the `tkn pac bootstrap](../../guide/cli/#bootstrap) command.

* **`version`**:  The version of Pipelines-as-Code you're running. Good to know!

* **`controller-url`**: The URL of the Pipelines-as-Code controller. This is set by `tkn pac bootstrap` when you set up the GitHub App (or during regular installation). The OpenShift Pipelines Operator automatically sets this to the route it creates for the controller.  `tkn pac webhook add` also uses this to find the controller URL.

* **`provider`**:  If you used `tkn pac bootstrap` with a GitHub App, this will be set to `GitHub App`.  It helps `tkn pac bootstrap` and `webhook add` figure out if a GitHub App is already configured.

## Logging - Get Verbose (or Not)

Pipelines-as-Code uses a ConfigMap named `pac-config-logging` (in the `pipelines-as-code` namespace) for its logging settings.  To see it, run:

```bash
$ kubectl get configmap pac-config-logging -n pipelines-as-code

NAME                 DATA   AGE
pac-config-logging   4      9m44s
```

To see what's inside:

```bash
$ kubectl get configmap pac-config-logging -n pipelines-as-code -o yaml

apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: default
    app.kubernetes.io/part-of: pipelines-as-code
  name: pac-config-logging
  namespace: pipelines-as-code
data:
  loglevel.pac-watcher: info
  loglevel.pipelines-as-code-webhook: info
  loglevel.pipelinesascode: info
  zap-logger-config: |
    {
      "level": "info",
      "development": false,
      "sampling": {
        "initial": 100,
        "thereafter": 100
      },
      "outputPaths": ["stdout"],
      "errorOutputPaths": ["stderr"],
      "encoding": "json",
      "encoderConfig": {
        "timeKey": "ts",
        "levelKey": "level",
        "nameKey": "logger",
        "callerKey": "caller",
        "messageKey": "msg",
        "stacktraceKey": "stacktrace",
        "lineEnding": "",
        "levelEncoder": "",
        "timeEncoder": "iso8601",
        "durationEncoder": "",
        "callerEncoder": ""
      }
    }
```

The `loglevel.*` entries control the log level for different Pipelines-as-Code components:

* `loglevel.pipelinesascode`:  For the main `pipelines-as-code-controller` component.
* `loglevel.pipelines-as-code-webhook`: For the `pipelines-as-code-webhook` component.
* `loglevel.pac-watcher`: For the `pipelines-as-code-watcher` component.

You can change these from `info` to `debug` (or other supported levels) to get more or less logging. For example, to set the `pac-watcher` to `debug`:

```bash
kubectl patch configmap pac-config-logging -n pipelines-as-code --type json -p '[{"op": "replace", "path": "/data/loglevel.pac-watcher", "value":"debug"}]'
```

After running this, the watcher will use the new log level.

Want to use the *same* log level for *all* Pipelines-as-Code components? Just remove the `loglevel.*` entries from the configmap:

```bash
kubectl patch configmap pac-config-logging -n pipelines-as-code --type json -p '[  {"op": "remove", "path": "/data/loglevel.pac-watcher"},  {"op": "remove", "path": "/data/loglevel.pipelines-as-code-webhook"},  {"op": "remove", "path": "/data/loglevel.pipelinesascode"}]'
```

In this case, all components will use the default log level set in `zap-logger-config` (the `"level": "info"` line in the JSON).

The `zap-logger-config` supports these log levels:

* `debug`: Super detailed debugging info.
* `info`: Normal, everyday logging.
* `warn`: Something unexpected happened, but it's probably not critical.
* `error`: A serious error occurred – something went wrong during normal operation.
* `dpanic`: In development mode, this will cause a panic (crash).
* `panic`:  Causes a panic (crash).
* `fatal`: Immediately exits with an error (exit code 1).

Want to learn more about these log levels? Check out: <https://knative.dev/docs/serving/observability/logging/config-logging>
